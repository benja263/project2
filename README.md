# README

This file contains documentation of how to run the code, details about the code base and the implementation.

Our final submission was computed using the word2vec cnn model.

## Requirements:
### Technical requirements:
This project was written in Python 3.6.1.
The following python libraries must be installed to run the code:
```
fasttext (fastText python interface)
h5py
keras
matplotlib
nltk
numpy
scikit-learn
tensorflow
pickle
pandas
```
Note: ```keras``` should be installed after tensforflow

These libraries can be installed using ```pip ```. For example:
```
$ pip install fasttext
```

In addition, the ```sed``` utility and ```grep``` utility are required, as well as the ability to run bash scripts, so we recommend using a UNIX system.

Finally, a computer with at least 16GB of RAM is recommended.

### Other requirements:
The tweets dataset is not included in this project (for the sake of size). The following files must be in ```final/data/raw/``` before running the code (files can be found here in ```twitter_data.zip``` at https://drive.switch.ch/index.php/s/TmKwo9UNqfkTvfp) :

```
test_data.txt
train_neg.txt
train_neg_full.txt
train_pos.txt
train_pos_full.txt
```

## Codebase:
Our codebase is split into two main directories: ```data/``` and ```src/```.

```data/ ``` contains the dataset we used. It has two subdirectories: ```raw``` and ```preprocessed```:
- ```raw``` contains the twitter data set. It is decomposed into 4 training files ```train_pos.txt```, ```train_neg.txt```, ```train_pos_full.txt```, ```train_neg_full.txt``` and one test file ```test_data.txt``` (Note that the directory is empty here for the sake of size. The user should copy the files mentioned in the requirements section).
- ```preprocessed``` contains the preprocessed dataset, in particular it contains only the file ```parsed_test_data.txt```. Other preprocessed files are generated directly in their respective model's folder.

```src/``` contains the code of the different models we used. It has three subdirectories ```baseline```, ```cnn``` and ```fasttext```.

### Baseline
Running instructions:
```
cd final/src/baseline
sh build_data.sh
python3 run.py
```

The code takes around 2 hours. If you don't want to wait, you can directly put the content of ```baseline_data.zip``` in ```final/src/baseline/``` and run ```run.py```.

In the baseline model, we used the GloVe vector representation model code given for this project. The idea here is to  generate word embeddings by training on the training set corpus, then compute the word representation of each tweet by averaging the vector of its words. Finally, training a classifier on the converted training set of tweets.

We used ```sed``` and ```grep``` to construct a vocabulary. ```glove_solution.py``` creates the embeddings.

Steps:
- Building the vocabulary (using sed)
- Filtering out words appearing less than 5 times
- Computing the matrix of co-occurrence (in ```cooc.py```)
- Generating the word embeddings using the GloVe algorithm (in ```glove_solution.py```)
- Computing the GloVe representation of a tweet as the average of its words' vector, for both training and testing set (in ```build_test_feauture_matrix.py``` and ```build_train_feature_matrix.py```).
- Training a linear classifier on the transformed tweets and doing cross validation (in ```validation.py```)
- Classifying using the trained model (in ```run.py```)

### Convolutional Neural Nets
In the CNN we use 4 pre-trained models:
GloVe 300 embedding dimension 2014 Wikipedia pre-trained model that can be downloaded from here: http://nlp.stanford.edu/data/glove.6B.zip (the extracted file (glove.6B.300d.txt)  should be in the ```final/src/cnn/```. Then run ```create_glove_vocab.py```  which will create the vocabulary ```glove_dict.pickle```.
Fasttext generated pre-embedding word vectors either using CBOW SKIPGRAM
    they can be generated by:
	Google’s Word2Vec pre-trained word-vector that can be downloaded from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing  and extract the file (GoogleNews-vectors-negative300.bin) to the ```final/src/cnn/``` folder.

	Running instructions: (make sure to download the data from the download links before)
	```
	cd final/src/cnn/
	sh build_data.sh
	python3 word2vec_run.py 
	python3 word2vec_predict.py
	```
	The ```word2vec_run.py```and ```word2vec_predict.py``` can be replaced with the other run and predict model files.
	Note: The ```fasttext_predict.py``` and ```fasttext_run.py``` have 2 options either use the cbow or the skipgram model (their test corpus is identical) with instructions in the files on how to use them (commenting and uncommenting of lines).

	If you don't want to build the data, you can download ```GoogleNews-vectors-negative300.bin```, ```cbow_dict.pickle```, ```skipgram_dict.pickle```, ```glove_dict.pickle``` and put them in ```final/src/cnn/```. You still need to call ```*_run.py``` and then ```*_predict.py```.

	If you don't want to train the model, you can download the models we trained (```models.zip```), their history files (```modelhistories.zip```), training corpuses (```test_corpuses.zip```) and put them in ```final/src/cnn```. Now you can call ```*_predict.py```.

	All the files can be found on https://drive.switch.ch/index.php/s/TmKwo9UNqfkTvfp.

	Note: the ```*_run.py``` files take a long time to run (each epoch takes: ~1000 seconds for fasttext models, ~2500 seconds for GloVe and Word2Vec resulting in up to 10.5 hours for 15 epochs).

	All CNN models clean their tweets with a downloaded english text normalization dictionary (this is done in the ```pre_process_text.py```). The dictionary comes from: http://www.hlt.utdallas.edu/~yangl/data/Text_Norm_Data_Release_Fei_Liu/

	The ```*_run.py``` files output is the history files + test_corpuses files + model files (.h5 files). the history files ```final_ft_history_cbow.pickle```, ```final_ft_history_skipgram.pickle```, ```final_glove_history.pickle```, final_Word2vec_history.pickle) are needed to create the graphs using ```graphs.py```. 
	The test corpuses (```test_corpus_glove.pickle```, ```test_corpus_fasttext.pickle``` and ```test_corpus_word2vec.pickle```) and the generated models (or downloaded models), are used in the ```*_predict.py``` files (```glove_predict.py```, ```fasttext_predict.py```, ```word2vec_predict.py```) . The ```*_predict.py``` files are used to generate the csv submission files that were uploaded to the kaggle competition for scoring. If one has downloaded our runned models and test corpuses one can directly use the ```*_predict.py``` files to generate the submissions we used for our kaggle score.

	How keras builds the embedding matrix:
	Keras uses the tokenizer function on the training set to create a vocabulary linking each word with an index. Then it replaces each tweet by a sequence. A sequence is a representation of a tweet where the words are changed by their respective indexes in the created vocabulary. The downloaded word-vector data matrices are not indexed in relation to the vocabulary, the ```transform_tweet.py``` takes a downloaded matrix and a created vocabulary and links them by creating a new embedding matrix with dimensions (Number of words in the vocabulary X width of word-vector matrix). The link is done by finding the index of the word in the vocabulary and if the word exists in the downloaded matrix it puts its word-vector in the row corresponding to the vocabulary index, resulting in a new embedding matrix where each row index corresponds to each vocabulary index. Words who do not appear in the downloaded matrix are represented by a zero vector. This is important as the sequence representation for a tweets depends on these indexes.
	This new embedding matrix is used by the CNN in order to predict the sentiment of a tweet.

	### FastText
	Running instructions:
	```
	cd final/src/fasttext/
	sh build_data.sh
	python3 run.py
	```

	The code takes around less than 30 minutes.

	In the fastText model, we used the fastText library classifier. First, we tried to do classification without altering the dataset. Then, we tried to clean the tweets by removing special character and  finally cleaning the tweets by removing special character and stop words.

	We removed special characters from tweets using the python re expression. 

	For the stop words, we used the the list given by the nltk library. Filtering the stop words was done by iterating through the tweets and keeping only the words not appearing in the stop words list.

	The fastText classifier requires each training data instance to be labeled with the prefix “__label__<label>”. In our case we add “__label__1” for the positive tweets and “__label__-1” for the negative tweets.

	The ```plot.py``` contains the code of the different plots.

	Steps:
	- Generate the filtered test datasets (filtering special characters and filtering special characters + stopwords in ```generate_filtered_datasets.py```) 
	- Format the resulting datasets to be usable by the fasttext classifier (in ```format_data.py```).
	- Training the classifier on one of the dataset and classifying using the trained model (in ```run.py```).

